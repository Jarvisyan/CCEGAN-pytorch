{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.ones(1).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4782,
     "status": "ok",
     "timestamp": 1657715135746,
     "user": {
      "displayName": "yan jingle",
      "userId": "14928584207358225609"
     },
     "user_tz": -480
    },
    "id": "oEkWnByu97Iv",
    "outputId": "b3755df6-4972-4cd6-f8cd-f1d20610c0e4"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1657715135747,
     "user": {
      "displayName": "yan jingle",
      "userId": "14928584207358225609"
     },
     "user_tz": -480
    },
    "id": "AIlOcQ68-F5n",
    "outputId": "fb3ec7f3-33b2-4468-d75e-1ad19cf50701"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd()) \n",
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install d2l\n",
    "# !pip install munkres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3223,
     "status": "ok",
     "timestamp": 1657715138966,
     "user": {
      "displayName": "yan jingle",
      "userId": "14928584207358225609"
     },
     "user_tz": -480
    },
    "id": "P3zJjekY-QgO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import torch\n",
    "import random\n",
    "#import lightly\n",
    "import label_map\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from scipy import stats\n",
    "from d2l import torch as d2l\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "#import seaborn as sns; sns.set_theme()\n",
    "# from lightly.models.modules.heads import SimSiamProjectionHead\n",
    "# from lightly.models.modules.heads import SimSiamPredictionHead\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1657715138967,
     "user": {
      "displayName": "yan jingle",
      "userId": "14928584207358225609"
     },
     "user_tz": -480
    },
    "id": "Y9NPoTw5oJfk"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, class_num, z_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = z_dim + class_num\n",
    "        self.output_dim = 16 \n",
    "        \n",
    "        #FC_block: 15 -> 256 -> 256 -> 16\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(256, self.output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.fc(X) \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, class_num):\n",
    "        super().__init__()\n",
    "        self.class_num = class_num\n",
    "        \n",
    "        #FC_block: 16 -> 256 -> 256 -> class_num+1\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(256, class_num + 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.fc(X) \n",
    "\n",
    "class E_net(nn.Module):\n",
    "    def __init__(self, class_num):\n",
    "        super().__init__()\n",
    "        self.class_num = class_num\n",
    "        \n",
    "        #FC_block: 16 -> 256 -> 256 -> class_num\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(256, class_num),\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.fc(X) \n",
    "\n",
    "class SimSiam(nn.Module):\n",
    "    def __init__(\n",
    "        self, pred_hidden_dim, out_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(16, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(256, out_dim),                \n",
    "        )\n",
    "        \n",
    "        self.prediction_head = nn.Sequential(\n",
    "            nn.Linear(out_dim, pred_hidden_dim),\n",
    "            nn.BatchNorm1d(pred_hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(pred_hidden_dim, out_dim),                \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # get projections\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        z = nn.Softmax(dim=1)(z)\n",
    "        H1 = (-1 * z * z.log()).sum() / z.shape[0] #average entropy\n",
    "        emperical_z = z.mean(0)\n",
    "        H2 = (-1 * emperical_z * emperical_z.log()).sum()\n",
    "        # get predictions\n",
    "        p = self.prediction_head(z)\n",
    "        # stop gradient\n",
    "        z = z.detach()\n",
    "        return z, p, H1, H2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1657715138967,
     "user": {
      "displayName": "yan jingle",
      "userId": "14928584207358225609"
     },
     "user_tz": -480
    },
    "id": "juLAREzVoJit"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(0, 0.02)\n",
    "        m.bias.data.zero_()      \n",
    "\n",
    "def gen_cond_label(batch_size, class_num, z_dim):\n",
    "    conditional_label = torch.zeros(batch_size, class_num)\n",
    "    cluster_size = round(batch_size / class_num)\n",
    "    for i in range(class_num):\n",
    "        if i == class_num - 1:\n",
    "            conditional_label[i * cluster_size : , i] = 1\n",
    "        else:\n",
    "            conditional_label[i * cluster_size : (i + 1) * cluster_size, i] = 1\n",
    "    G_input = torch.cat([conditional_label, torch.rand(batch_size, z_dim)], 1)\n",
    "    return G_input, conditional_label\n",
    "\n",
    "def Accuracy(true_label, pred_label):\n",
    "  k_set = torch.unique(true_label)\n",
    "  correct_num = 0\n",
    "  for i in k_set:\n",
    "      idx = true_label == i\n",
    "      cluster_i = pred_label[idx]\n",
    "      correct_num += torch.max(torch.bincount(cluster_i.int())) \n",
    "  accuracy = correct_num / len(true_label)\n",
    "  return float(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1244,
     "status": "ok",
     "timestamp": 1657715140207,
     "user": {
      "displayName": "yan jingle",
      "userId": "14928584207358225609"
     },
     "user_tz": -480
    },
    "id": "cOV5v9FXv1Ko"
   },
   "outputs": [],
   "source": [
    "def train_GAN(D, G, E, optimizer_D, optimizer_G, torch_dataset, class_num, z_dim, first_epoch, m, epoch, device):\n",
    "  for epoch_GAN in range(5):\n",
    "    metric = d2l.Accumulator(3)\n",
    "    train_iter = torch.utils.data.DataLoader(dataset = torch_dataset, batch_size = 64,\n",
    "                        shuffle = True, num_workers = 4)\n",
    "    num_batches = len(train_iter)\n",
    "    for i, (X, _) in enumerate(train_iter):\n",
    "      #train D first\n",
    "      optimizer_D.zero_grad()\n",
    "      X = X.to(device)\n",
    "      X_size = X.shape[0]\n",
    "      #real_images_loss \n",
    "      D_real = D(X)\n",
    "      if first_epoch:\n",
    "        Y_real = 1/class_num * torch.ones(X_size, class_num, device = device)\n",
    "      else:\n",
    "        Y_real = nn.Softmax(dim = 1)(E(X)).detach()\n",
    "      Y_real = torch.cat([Y_real, 2 * torch.ones(X_size, 1, device = device)], 1)\n",
    "      ones = torch.ones_like(Y_real)\n",
    "      ones[:, -1] = 0\n",
    "      D_real_loss = torch.nn.BCEWithLogitsLoss(weight = Y_real)(D_real, ones)\n",
    "      \n",
    "      #fake_images_loss\n",
    "      G_input, _ = gen_cond_label(X_size, class_num, z_dim)\n",
    "      G_input = G_input.to(device)\n",
    "      X_fake = G(G_input).detach() \n",
    "      D_fake = D(X_fake)\n",
    "      Y_fake = torch.zeros_like(Y_real)\n",
    "      Y_fake[:, -1] = 1\n",
    "      D_fake_loss = torch.nn.BCEWithLogitsLoss()(D_fake, Y_fake)\n",
    "\n",
    "      #total loss\n",
    "      D_loss = D_real_loss + D_fake_loss\n",
    "      D_loss.backward()\n",
    "      optimizer_D.step()\n",
    "      \n",
    "      #train G\n",
    "      #G.train()\n",
    "      #D.eval()\n",
    "      optimizer_G.zero_grad()\n",
    "\n",
    "      G_input, cond_label = gen_cond_label(X_size, class_num, z_dim)\n",
    "      G_input = G_input.to(device)\n",
    "      X_fake = G(G_input)\n",
    "      D_fake = D(X_fake)\n",
    "      Y_fake = torch.cat([cond_label, torch.zeros(X_size, 1)], 1)\n",
    "      Y_fake = Y_fake.to(device)\n",
    "\n",
    "      G_loss = torch.nn.BCEWithLogitsLoss()(D_fake, Y_fake)\n",
    "      G_loss.backward()\n",
    "      optimizer_G.step()\n",
    "      \n",
    "\n",
    "\n",
    "def train_simsiam(Nets, my_SimSiam, optimizer_my_SimSiam, cluster_size, class_num, iterations, z_dim, M, epoch, animator, device):\n",
    "\n",
    "    fake_num = cluster_size * class_num\n",
    "    for m in range(M):\n",
    "        Nets[f'fake_samples_{m}'] = torch.zeros(0, device = device)\n",
    "        Nets[f'fake_labels_{m}'] = torch.zeros(0, device = device)\n",
    "\n",
    "    metric = d2l.Accumulator(4)\n",
    "    for iter in range(iterations):   \n",
    "        #generate\n",
    "        X_to_fuse = torch.zeros(0, device = device)   \n",
    "        for m in range(M):\n",
    "            G_input, cond_label = gen_cond_label(fake_num, class_num, z_dim)\n",
    "            pseudo_label = torch.argmax(cond_label, 1)\n",
    "            G_input, pseudo_label = G_input.to(device), pseudo_label.to(device)\n",
    "            X_fake = Nets[f'G_{m}'](G_input).detach()\n",
    "            # X_fake = Nets[f'G_{anchor_idx}'](G_input).detach()\n",
    "          \n",
    "            #save\n",
    "            X_to_fuse = torch.cat([X_to_fuse, X_fake], 0)\n",
    "            Nets[f'fake_samples_{m}'] = torch.cat([Nets[f'fake_samples_{m}'], X_fake], 0)\n",
    "            Nets[f'fake_labels_{m}'] = torch.cat([Nets[f'fake_labels_{m}'], pseudo_label], 0)\n",
    "\n",
    "        #contrast learning\n",
    "        my_SimSiam.train()\n",
    "        optimizer_my_SimSiam.zero_grad()\n",
    "        z, p, H1, H2 = my_SimSiam(X_to_fuse)\n",
    "        my_SimSiam_loss = 0\n",
    "        for m in range(M):\n",
    "            for i in range(class_num):\n",
    "                idx1, idx2 = cluster_size * i + fake_num * m, cluster_size * (i + 1) + fake_num * m\n",
    "                z_i, p_i = z[idx1 : idx2], p[idx1 : idx2]\n",
    "                z_i_norm, p_i_norm = F.normalize(z_i), F.normalize(p_i)\n",
    "                my_SimSiam_loss -= torch.mm(z_i_norm, p_i_norm.T).sum() / (cluster_size ** 2)\n",
    "        total_loss = my_SimSiam_loss + 15 * M * (H1 - H2)\n",
    "        total_loss.backward()\n",
    "        optimizer_my_SimSiam.step()\n",
    "\n",
    "        #record loss\n",
    "        with torch.no_grad():\n",
    "          metric.add(my_SimSiam_loss, H1, H2, 1)\n",
    "        my_SimSiam_loss = metric[0] / metric[3]\n",
    "        H1 = metric[1] / metric[3]\n",
    "        H2 = metric[2] / metric[3]\n",
    "        if (iter + 1) % 100 == 0 or iter == 0:\n",
    "          # with open('output/pendigits/results/CCEGAN/r2/log_loss.txt', 'a') as f: #save output\n",
    "          #   f.write(f'epoch_iteration = {5 * (epoch + 1)}_{iter + 1}, my_SimSiam_loss = {my_SimSiam_loss}, H1 = {H1}, H2 = {H2}\\n')\n",
    "          animator.add(5 * (epoch + (iter + 1) / iterations), (my_SimSiam_loss, H1, H2))\n",
    "    ##evaluate by SIMSIAM\n",
    "    SIMSIAM_flag = True\n",
    "    eval_by_E(my_SimSiam, 'my_SimSiam', epoch, torch_dataset, SIMSIAM_flag, device)\n",
    "          \n",
    "def eval_by_E(E, m, epoch, torch_dataset, SIMSIAM_falg, device):\n",
    "  E.eval()\n",
    "  pred_label = torch.zeros(0, device = device)\n",
    "  true_label = torch.zeros(0)\n",
    "  eval_iter = torch.utils.data.DataLoader(dataset = torch_dataset, batch_size = 2000,\n",
    "                    shuffle = False, num_workers = 4)\n",
    "  for X, y in eval_iter:\n",
    "    X = X.to(device)\n",
    "    if SIMSIAM_falg:\n",
    "      pred = E(X)[0].detach()\n",
    "    else:\n",
    "      pred = nn.Softmax(dim =1)(E(X)).detach()\n",
    "    label = torch.argmax(pred, 1)\n",
    "    pred_label = torch.cat([pred_label, label])\n",
    "    true_label = torch.cat([true_label, y])\n",
    "  pred_label = pred_label.to('cpu')\n",
    "  accuracy = Accuracy(true_label, pred_label)\n",
    "  nmi = NMI(true_label, pred_label)\n",
    "  print(f'E_{m}: {accuracy}, {nmi}')\n",
    "  # if SIMSIAM_falg and (epoch + 1) % 4 == 0:\n",
    "  #     torch.save((true_label.type(torch.LongTensor), pred_label.type(torch.LongTensor)),\n",
    "  #           f'output/pendigits/results/CCEGAN/r2/labels_{5 * (epoch + 1)}')\n",
    "  with open('performance.txt', 'a') as f:\n",
    "      f.write(f'E_base, round = {(epoch + 1)}, E_{m}, Accuracy = {accuracy}, NMI = {nmi}\\n')\n",
    "\n",
    "def filter_images(my_SimSiam, fake_samples, fake_labels, class_num, cluster_size_chosen, device):\n",
    "    my_SimSiam.eval()\n",
    "    fake_samples_filtered = torch.zeros(0, device = device)\n",
    "    cluster_size = []\n",
    "    for i in range(class_num): \n",
    "        cluster_idx = fake_labels == i\n",
    "        images_cluster_i = fake_samples[cluster_idx]\n",
    "        cluster_i_size = images_cluster_i.shape[0]\n",
    "        \n",
    "        pred_conf = torch.zeros((cluster_i_size, class_num), device = device)\n",
    "        pred_label = torch.zeros(cluster_i_size)\n",
    "        batch_num = int(cluster_i_size / 500)\n",
    "        for i in range(batch_num):\n",
    "            idx1, idx2 = 500 * i, 500 * (i + 1)\n",
    "            pred = my_SimSiam(images_cluster_i[idx1 : idx2])[0].detach()\n",
    "            label = torch.argmax(pred, 1)\n",
    "            pred_conf[idx1 : idx2] = pred\n",
    "            pred_label[idx1 : idx2] = label\n",
    "        pred_conf = pred_conf.cpu()\n",
    "        label_mode, _ = pred_label.mode()\n",
    "        \n",
    "        idx_chosen = pred_label == label_mode\n",
    "        pred_conf = pred_conf[idx_chosen]\n",
    "        images_cluster_i = images_cluster_i[idx_chosen]\n",
    "        mode_num = sum(idx_chosen)\n",
    "        if mode_num > cluster_size_chosen:\n",
    "            v, _ = pred_conf.max(dim = 1)\n",
    "            _, idx_high = v.sort(descending = True)\n",
    "            idx_high_chosen = idx_high[:cluster_size_chosen]\n",
    "            images_cluster_i_chosen = images_cluster_i[idx_high_chosen]\n",
    "            fake_samples_filtered = torch.cat([fake_samples_filtered, images_cluster_i_chosen], 0)\n",
    "            cluster_size.append(cluster_size_chosen)\n",
    "        else:\n",
    "            fake_samples_filtered = torch.cat([fake_samples_filtered, images_cluster_i], 0)\n",
    "            cluster_size.append(mode_num)\n",
    "    return fake_samples_filtered, torch.tensor(np.repeat(range(class_num), cluster_size), device = device).long()\n",
    "\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1657715140207,
     "user": {
      "displayName": "yan jingle",
      "userId": "14928584207358225609"
     },
     "user_tz": -480
    },
    "id": "Wf8Tt25YVbHU"
   },
   "outputs": [],
   "source": [
    "def firt_filter(my_SimSiam, Nets, torch_dataset, cluster_size_chosen, M, epoch, device):\n",
    "    fake_samples_filtered_all = torch.zeros(0, device = device)\n",
    "    fake_samples_num_ls = []\n",
    "  \n",
    "    for m in range(M):\n",
    "        fake_samples_filtered, fake_labels_filtered = filter_images(my_SimSiam, Nets[f'fake_samples_{m}'], Nets[f'fake_labels_{m}'],\n",
    "                                      class_num, cluster_size_chosen, device)\n",
    "        fake_samples_filtered_all = torch.cat([fake_samples_filtered_all, fake_samples_filtered], 0)\n",
    "        fake_samples_num_ls.append(fake_samples_filtered.shape[0])\n",
    "        torch_fake = torch.utils.data.TensorDataset(fake_samples_filtered, fake_labels_filtered)\n",
    "        # with open('output/pendigits/results/CCEGAN/r2/log_loss.txt', 'a') as f:\n",
    "        #     f.write('\\n')\n",
    "        for iter in range(3):\n",
    "          count = 0\n",
    "          train_fake = torch.utils.data.DataLoader(dataset = torch_fake, batch_size = 256, shuffle = True, num_workers = 0)\n",
    "          batch_num = len(train_fake)\n",
    "          for X_fake, y_fake in train_fake:\n",
    "            Nets[f'optimizer_E_{m}'].zero_grad()\n",
    "            pred = Nets[f'E_{m}'](X_fake)\n",
    "            E_loss = torch.nn.CrossEntropyLoss()(pred, y_fake)\n",
    "            E_loss.backward()\n",
    "            Nets[f'optimizer_E_{m}'].step()\n",
    "            with torch.no_grad():\n",
    "              count += E_loss\n",
    "          E_loss = count / batch_num\n",
    "          # with open('output/pendigits/results/CCEGAN/r2/log_loss.txt', 'a') as f:\n",
    "          #   f.write(f'epoch_iteration = {5 * (epoch + 1)}_{iter + 1}, E_{m}_loss = {E_loss}\\n')\n",
    "        ##evaluate by Es\n",
    "        SIMSIAM_flag = False\n",
    "        eval_by_E(Nets[f'E_{m}'], m, epoch, torch_dataset, SIMSIAM_flag, device)\n",
    "    return fake_samples_filtered_all, fake_samples_num_ls\n",
    "\n",
    "def second_filter(Nets, fake_samples_filtered_all, fake_samples_num_ls, first_epoch, M, epoch, device):\n",
    "    base_partitions = torch.zeros(0)\n",
    "    #generate pseudo_labels\n",
    "    fake_iter = torch.utils.data.DataLoader(dataset = fake_samples_filtered_all, batch_size = 2000,\n",
    "                        shuffle = False, num_workers = 0)\n",
    "    for m in range(M):\n",
    "        #label assignment by base E\n",
    "        pred = label_assignment_by_base_E(Nets[f'E_{m}'], fake_iter, device)\n",
    "        base_partitions = torch.cat([base_partitions, pred.reshape(-1, 1)], 1)\n",
    "        base_partitions = base_partitions.type(torch.LongTensor)\n",
    "    #label alignment\n",
    "    base_fake_aligned = torch.zeros_like(base_partitions)\n",
    "    anchor = base_partitions[:, 0]\n",
    "    base_fake_aligned[:, 0] = anchor\n",
    "    if first_epoch: \n",
    "        for m in range(1, M):\n",
    "            base = base_partitions[:, m]\n",
    "            base_aligned, base_to_anchor, anchor_to_base = label_map.label_map(anchor, base)\n",
    "            base_fake_aligned[:, m] = torch.tensor(base_aligned)\n",
    "            Nets[f'Base_to_anchor_{m}'] = base_to_anchor #save map\n",
    "            Nets[f'Anchor_to_base_{m}'] = anchor_to_base #save map\n",
    "    else:\n",
    "        for m in range(1, M):\n",
    "            base = base_partitions[:, m]\n",
    "            for i in range(class_num):\n",
    "                idx_base = base == i\n",
    "                base_fake_aligned[:, m][idx_base] = Nets[f'Base_to_anchor_{m}'][i]\n",
    "    label_fused, count_vote = stats.mode(base_fake_aligned, axis = 1) #voted pred_label\n",
    "    idx_high_confidence = (count_vote > M/2).squeeze()\n",
    "\n",
    "\n",
    "    SIMSIAM_flag = False\n",
    "    idx_start, idx_end = 0, 0\n",
    "    for m in range(M):\n",
    "        Nets[f'E_{m}'].train()\n",
    "        idx_end += fake_samples_num_ls[m] \n",
    "        fake_samples_chosen = fake_samples_filtered_all[idx_start : idx_end]\n",
    "        label_fused_chosen = label_fused[idx_start : idx_end]\n",
    "        label_reset = np.zeros_like(label_fused_chosen)\n",
    "        if m == 0:\n",
    "            label_reset = label_fused_chosen.squeeze()\n",
    "        else:\n",
    "            for i in range(class_num):\n",
    "                idx_cluster = label_fused_chosen == i\n",
    "                label_reset[idx_cluster] = Nets[f'Anchor_to_base_{m}'][i]\n",
    "            label_reset = label_reset.squeeze()\n",
    "        idx_high_confidence_chosen = idx_high_confidence[idx_start : idx_end]\n",
    "        fake_dataset = torch.utils.data.TensorDataset(fake_samples_chosen[idx_high_confidence_chosen],\n",
    "                                torch.tensor(label_reset)[idx_high_confidence_chosen])\n",
    "        count = 0\n",
    "        for _ in range(2):\n",
    "          fake_iter = torch.utils.data.DataLoader(dataset = fake_dataset,\n",
    "                            batch_size = 128, shuffle = True, num_workers = 0)\n",
    "          for X, y in fake_iter:\n",
    "              X, y = X.to(device), y.to(device)\n",
    "              Nets[f'optimizer_E_{m}'].zero_grad()\n",
    "              pred = Nets[f'E_{m}'](X)\n",
    "              E_loss = torch.nn.CrossEntropyLoss()(pred, y)\n",
    "              E_loss.backward()\n",
    "              Nets[f'optimizer_E_{m}'].step()\n",
    "              count += E_loss #save E_loss\n",
    "        idx_start = idx_end\n",
    "        eval_by_E(Nets[f'E_{m}'], m, epoch, torch_dataset, SIMSIAM_flag, device)\n",
    "    \n",
    "def label_assignment_by_base_E(E, train_iter, device):\n",
    "    E.eval()\n",
    "    pred_label = torch.zeros(0, device = device)\n",
    "    for X in train_iter:\n",
    "        X = X.to(device)\n",
    "        pred = nn.Softmax(dim =1)(E(X)).detach()\n",
    "        label = torch.argmax(pred, 1)\n",
    "        pred_label = torch.cat([pred_label, label], 0)\n",
    "    return pred_label.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1657715140208,
     "user": {
      "displayName": "yan jingle",
      "userId": "14928584207358225609"
     },
     "user_tz": -480
    },
    "id": "IUgMDZzYImky"
   },
   "outputs": [],
   "source": [
    "def train_CCEGAN(torch_dataset, M, class_num, z_dim, num_epochs, device):\n",
    "    #setup_seed(20)\n",
    "    out_dim = class_num\n",
    "    pred_hidden_dim = 64\n",
    "    my_SimSiam = SimSiam(pred_hidden_dim, out_dim)\n",
    "    my_SimSiam.to(device)\n",
    "    optimizer_my_SimSiam = torch.optim.Adam(my_SimSiam.parameters(), lr = 6e-5, betas = (0.5, 0.99), weight_decay = 2.5 * 1e-5)\n",
    "\n",
    "    Nets = locals()  \n",
    "    for m in range(M):\n",
    "        Nets[f'G_{m}'] = Generator(class_num, z_dim)\n",
    "        Nets[f'D_{m}'] = Discriminator(class_num)\n",
    "        Nets[f'E_{m}'] = E_net(class_num)\n",
    "\n",
    "        Nets[f'G_{m}'].apply(init_weights)\n",
    "        Nets[f'D_{m}'].apply(init_weights)\n",
    "        Nets[f'E_{m}'].apply(init_weights)\n",
    "        \n",
    "        Nets[f'G_{m}'].to(device) \n",
    "        Nets[f'D_{m}'].to(device) \n",
    "        Nets[f'E_{m}'].to(device)\n",
    "\n",
    "        Nets[f'optimizer_G_{m}'] = torch.optim.Adam(Nets[f'G_{m}'].parameters(),\n",
    "                          lr = 3e-4, betas = (0.5, 0.9), weight_decay = 2.5 * 1e-5)\n",
    "        Nets[f'optimizer_D_{m}'] = torch.optim.Adam(Nets[f'D_{m}'].parameters(),\n",
    "                          lr = 6e-4, betas = (0.5, 0.9), weight_decay = 2.5 * 1e-5)\n",
    "        Nets[f'optimizer_E_{m}'] = torch.optim.Adam(Nets[f'E_{m}'].parameters(),\n",
    "                          lr = 3e-5, betas = (0.5, 0.9), weight_decay = 2.5 * 1e-5)\n",
    "\n",
    "\n",
    "\n",
    "    first_epoch = True \n",
    "    animator = d2l.Animator(xlabel = 'iter', xlim = [1, num_epochs * 5], \n",
    "            legend = ['my_SimSiam_loss', 'H1', 'H2'])\n",
    "    for epoch in range(num_epochs):\n",
    "        for m in range(M):\n",
    "            Nets[f'G_{m}'].train()\n",
    "            Nets[f'D_{m}'].train()\n",
    "            Nets[f'E_{m}'].train()\n",
    "            \n",
    "            #train model\n",
    "            ##train GAN first\n",
    "            train_GAN(Nets[f'D_{m}'], Nets[f'G_{m}'], Nets[f'E_{m}'], Nets[f'optimizer_D_{m}'], Nets[f'optimizer_G_{m}'], \n",
    "                torch_dataset, class_num, z_dim, first_epoch, m, epoch, device)\n",
    "       \n",
    "        #train SimSiam\n",
    "        cluster_size = 4  #M = 5\n",
    "        iterations = 1000\n",
    "        train_simsiam(Nets, my_SimSiam, optimizer_my_SimSiam, cluster_size, class_num, iterations, z_dim, M, epoch, animator, device)\n",
    "        \n",
    "        #train Es\n",
    "        ##filter images\n",
    "        ###first filter      \n",
    "        cluster_size_chosen = 500 * 5\n",
    "        fake_samples_filtered_all, fake_samples_num_ls = firt_filter(my_SimSiam, Nets, torch_dataset, cluster_size_chosen, M, epoch, device)\n",
    "        ###second filter\n",
    "        second_filter(Nets, fake_samples_filtered_all, fake_samples_num_ls, first_epoch, M, epoch, device)\n",
    "        \n",
    "        first_epoch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2305,
     "status": "ok",
     "timestamp": 1657715142511,
     "user": {
      "displayName": "yan jingle",
      "userId": "14928584207358225609"
     },
     "user_tz": -480
    },
    "id": "mNgQuj497ZJT"
   },
   "outputs": [],
   "source": [
    "finp_tr = 'pendigits/pendigits.tra.txt'\n",
    "finp_tes = 'pendigits/pendigits.tes.txt'\n",
    "data_tr = np.loadtxt(finp_tr, delimiter=',')\n",
    "X_train = data_tr[:, 0:16]\n",
    "X_train /= 100.0\n",
    "Y_train = data_tr[:, -1].astype(int)\n",
    "\n",
    "data_tes = np.loadtxt(finp_tes, delimiter=',')\n",
    "X_test = data_tes[:, 0:16]\n",
    "X_test /= 100.0\n",
    "Y_test = data_tes[:, -1].astype(int)\n",
    "\n",
    "X = np.concatenate((X_train, X_test))\n",
    "Y = np.concatenate((Y_train, Y_test)).astype(int)\n",
    "torch_dataset = torch.utils.data.TensorDataset(torch.tensor(X).float(), torch.tensor(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1657715152172,
     "user": {
      "displayName": "yan jingle",
      "userId": "14928584207358225609"
     },
     "user_tz": -480
    },
    "id": "VwRKXjdXdWDt"
   },
   "outputs": [],
   "source": [
    "z_dim, class_num, num_epochs, M = 5, 10, 12, 5\n",
    "device = d2l.try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "executionInfo": {
     "elapsed": 1735306,
     "status": "ok",
     "timestamp": 1657716890017,
     "user": {
      "displayName": "yan jingle",
      "userId": "14928584207358225609"
     },
     "user_tz": -480
    },
    "id": "cYi1kwda1R_f",
    "outputId": "b4ca8c44-eb36-4814-d7fc-5649a4574962"
   },
   "outputs": [],
   "source": [
    "#start = torch.cuda.Event(enable_timing=True)\n",
    "#end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "#start.record()\n",
    "train_CCEGAN(torch_dataset, M, class_num, z_dim, num_epochs, device)\n",
    "#end.record()\n",
    "\n",
    "# Waits for everything to finish running\n",
    "#torch.cuda.synchronize()\n",
    "\n",
    "#print(start.elapsed_time(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvHcSu181SC8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNhIMZSBuT+XHEs63HXrK5I",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "test3_CCEGAN_double filter_pendigtis_v1.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
